{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a598a45-7588-4524-a357-6086705513ba",
   "metadata": {},
   "source": [
    "### This is the pre-training of color representation model\n",
    "- It takes 5-10mins on GPU (Tesla T4 * 1) within 100 epochs for one time training\n",
    "  (reset batch-size=2048 in model_config)\n",
    "- For a quick start, do Pretraining with pre-created color corpus files\n",
    "- To create color corpus for training, read the metadata of multipalette and represent colors with bins\n",
    "    - In this work, we represent CIELab color data into 16 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2176a5ab-f5c1-4c32-97ef-b99a1556866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 16:46:59.900327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/colorbert')\n",
    "\n",
    "import color_bert_model as Model\n",
    "from input_data_generator import DataGenerator\n",
    "from model_config import Config\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81982788-7c8a-4d52-98c6-d710eb63c496",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create color corpus for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4a0bd6-5804-443d-840d-7bf53e5eaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent color with bins (bin_range = 16 <16bins> vocabulary size: max 4096)\n",
    "bin_range = Config['bin_range']\n",
    "representation = Config['representation']\n",
    "column_names = ['image_colors_lab', 'svg_colors_lab', 'text_colors_lab']\n",
    "\n",
    "def get_color_list_bins(data, column_names):\n",
    "    color_hist = ''\n",
    "    for column in column_names:\n",
    "        if pd.notna(data[column]):\n",
    "            colors = ast.literal_eval(data[column])\n",
    "            for color in colors:\n",
    "                if color_hist != '':\n",
    "                    color_hist += ' '\n",
    "                color_hist += f'{math.floor(color[0]/bin_range)}_{math.floor(color[1]/bin_range)}_{math.floor(color[2]/bin_range)}'\n",
    "    return color_hist\n",
    "\n",
    "def get_color_metadata(data, representation):\n",
    "\n",
    "    for column in column_names:\n",
    "        data[f'{column}'] = data.apply(lambda x: get_color_list_bins(x, [column]), axis=1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def get_color_hist(data, column_names):\n",
    "    color_hist = ''\n",
    "    color_hist += f'{data[column_names[0]]} ; {data[column_names[1]]} ; {data[column_names[2]]}'\n",
    "\n",
    "    return color_hist\n",
    "\n",
    "def create_colordata(file_path, representation):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    metadata = get_color_metadata(data, representation)\n",
    "    metadata['color_hist'] = metadata.apply(lambda x: get_color_hist(x, column_names), axis=1)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5423945-d7b3-4f42-a1b5-3ae7076a20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 796\n"
     ]
    }
   ],
   "source": [
    "dataTypes = ['train', 'val', 'test']\n",
    "data_path = '../data/training_data'\n",
    "for dataType in dataTypes:\n",
    "    metadata = create_colordata(f'{data_path}/metadata_colors/crello_colors_{dataType}_sklearn_lab.csv', representation)\n",
    "    metadata['color_hist'].to_csv(f'{data_path}/data_bert/data_color/color_corpus_lab_bins_16_{dataType}_sklearn.txt', header=None, index=None, sep=' ')\n",
    "    if dataType == 'train':\n",
    "        sentences = [row.split(' ') for row in metadata['color_hist']]\n",
    "        color_freq = defaultdict(int)\n",
    "        for sent in sentences:\n",
    "            for i in sent:\n",
    "                color_freq[i] += 1\n",
    "        color_freq.pop(';')\n",
    "        \n",
    "        colors = [a for a in color_freq]\n",
    "        print(f'vocabulary size: {len(colors)}')\n",
    "        with open(f'{data_path}/data_bert/data_color/color_vocab_lab_bins_16_{dataType}_sklearn.txt', 'w') as f:\n",
    "            f.write(\"[\")\n",
    "            for i in range(len(colors)):\n",
    "                f.write(\"'%s',\" % colors[i]) if i != len(colors) - 1 else f.write(\"'%s'\" % colors[i])\n",
    "            f.write(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e86c1-9967-4d93-9642-67e193892ee1",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63f47ed-5459-4f5f-bb3d-43c45dd73e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pretrain_task_accuracy(mlm_predict, batch_mlm_mask, origin_x):\n",
    "\n",
    "    batch_mlm_mask = tf.cast(batch_mlm_mask, dtype=tf.int32)\n",
    "    index = tf.where(batch_mlm_mask == 1)\n",
    "    x_predict = tf.math.argmax(mlm_predict, axis=-1) # top1\n",
    "    x_predict = tf.gather_nd(x_predict, index)\n",
    "    x_real = tf.gather_nd(origin_x, index)\n",
    "    mlm_accuracy = tf.keras.metrics.Accuracy()\n",
    "    mlm_accuracy.update_state(x_predict, x_real)\n",
    "    mlm_accuracy = mlm_accuracy.result().numpy()\n",
    "\n",
    "    return mlm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90846b68-b1ea-4857-ad2c-a5bc1c71f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 16:47:15.773648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 292, loss 0.7378, mlm_loss 0.7378, mlm_acc 0.1364\n",
      "Val: Epoch 0, step 35, loss 0.7569, mlm_loss 0.7569, mlm_acc 0.2273\n",
      "Epoch 1, step 292, loss 0.8263, mlm_loss 0.8263, mlm_acc 0.1481\n",
      "Val: Epoch 1, step 35, loss 0.8318, mlm_loss 0.8318, mlm_acc 0.0870\n",
      "Test: Epoch 1, step 34, loss 0.8415, mlm_loss 0.8415, mlm_acc 0.0800\n"
     ]
    }
   ],
   "source": [
    "# pretrain\n",
    "\n",
    "# training on CPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('CPU')\n",
    "assert len(physical_devices) > 0, \"Not enough CPU hardware devices available\"\n",
    "\n",
    "# training on GPU\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "model = Model.Bert(Config)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss_fn = Model.BERT_Loss()\n",
    "dataset = DataGenerator(Config)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(Config['saved_weight']))\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=Config['saved_weight'], max_to_keep=5)\n",
    "log_dir = os.path.join(Config['log_dir'], datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "\n",
    "# create the data for validation and test\n",
    "PROJECT_PATH = Config['project_path']\n",
    "Config_val = Config.copy()\n",
    "Config_val['corpus_file_path'] = os.path.join(PROJECT_PATH, f'data_color/color_corpus_lab_bins_16_val_sklearn.txt')\n",
    "dataset_val = DataGenerator(Config_val)\n",
    "\n",
    "Config_test = Config.copy()\n",
    "Config_test['corpus_file_path'] = os.path.join(PROJECT_PATH, f'data_color/color_corpus_lab_bins_16_test_sklearn.txt')\n",
    "dataset_test = DataGenerator(Config_test)\n",
    "\n",
    "patience = 30\n",
    "best = math.inf\n",
    "wait = 0\n",
    "\n",
    "for n in range(1):\n",
    "    EPOCH = 2 # 100 is enough\n",
    "    for epoch in range(EPOCH):\n",
    "    #     print(f'dataset length: {len(dataset)}')\n",
    "        for step in range(len(dataset)):\n",
    "            batch_x, batch_mlm_mask, batch_mcc_mask, origin_x, batch_segment, batch_padding_mask = dataset[step]\n",
    "            with tf.GradientTape() as t:\n",
    "                mlm_predict, sequence_output = model((batch_x, batch_mlm_mask, batch_segment), training=True)\n",
    "\n",
    "                mlm_loss = loss_fn((mlm_predict, batch_mlm_mask, origin_x))\n",
    "                mlm_loss = tf.reduce_mean(mlm_loss)\n",
    "\n",
    "                loss = mlm_loss\n",
    "\n",
    "            gradients = t.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # get acc of random mask\n",
    "            mlm_acc = calculate_pretrain_task_accuracy(mlm_predict, batch_mlm_mask, origin_x)\n",
    "\n",
    "            if step == len(dataset) - 1 and epoch % 1 == 0:\n",
    "                print(\n",
    "                    'Epoch {}, step {}, loss {:.4f}, mlm_loss {:.4f}, mlm_acc {:.4f}'.format(\n",
    "                        epoch, step, loss.numpy(),\n",
    "                        mlm_loss.numpy(),\n",
    "                        mlm_acc,\n",
    "                        ))\n",
    "\n",
    "        for val_step in range(len(dataset_val)):\n",
    "            val_batch_x, val_batch_mlm_mask, val_batch_mcc_mask, val_origin_x, val_batch_segment, val_batch_padding_mask = dataset_val[val_step]\n",
    "            val_mlm_predict, val_sequence_output = model((val_batch_x, val_batch_mlm_mask, val_batch_segment), training=False)\n",
    "\n",
    "            val_mlm_loss = loss_fn((val_mlm_predict, val_batch_mlm_mask, val_origin_x))\n",
    "            val_mlm_loss = tf.reduce_mean(val_mlm_loss)\n",
    "\n",
    "            # get acc of random mask\n",
    "            val_mlm_acc = calculate_pretrain_task_accuracy(val_mlm_predict, val_batch_mlm_mask, val_origin_x)\n",
    "\n",
    "            val_loss = val_mlm_loss\n",
    "\n",
    "            if val_step == len(dataset_val) - 1 and epoch % 1 == 0:\n",
    "                print(\n",
    "                    'Val: Epoch {}, step {}, loss {:.4f}, mlm_loss {:.4f}, mlm_acc {:.4f}'.format(\n",
    "                        epoch, val_step, val_loss.numpy(),\n",
    "                        val_mlm_loss.numpy(),\n",
    "                        val_mlm_acc,\n",
    "                        ))\n",
    "\n",
    "        path = manager.save(checkpoint_number=epoch)\n",
    "\n",
    "        # early stopping\n",
    "        wait += 1\n",
    "        if val_loss < best:\n",
    "            best = val_loss\n",
    "            wait = 0\n",
    "        if wait >= patience:\n",
    "            break\n",
    "\n",
    "    Config['mask_rate'] = 0\n",
    "    for test_step in range(len(dataset_test)):\n",
    "        test_batch_x, test_batch_mlm_mask, test_batch_mcc_mask, test_origin_x, test_batch_segment, test_batch_padding_mask = dataset_test[test_step]\n",
    "        test_mlm_predict, test_sequence_output = model((test_batch_x, test_batch_mlm_mask, test_batch_segment), training=False)\n",
    "\n",
    "        test_mlm_loss = loss_fn((test_mlm_predict, test_batch_mlm_mask, test_origin_x))\n",
    "        test_mlm_loss = tf.reduce_mean(test_mlm_loss)\n",
    "\n",
    "        # get acc of random mask\n",
    "        test_mlm_acc = calculate_pretrain_task_accuracy(test_mlm_predict, test_batch_mlm_mask, test_origin_x)\n",
    "\n",
    "        test_loss = test_mlm_loss\n",
    "\n",
    "        if test_step == len(dataset_test) - 1:\n",
    "            print(\n",
    "                'Test: Epoch {}, step {}, loss {:.4f}, mlm_loss {:.4f}, mlm_acc {:.4f}'.format(\n",
    "                    epoch, test_step, test_loss.numpy(),\n",
    "                    test_mlm_loss.numpy(),\n",
    "                    test_mlm_acc,\n",
    "                    ))\n",
    "\n",
    "    # model.save(f'../data/trained_models/bert_{representation}_{Config['mask_rate']}_{Config['mask_token_rate']}_{n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d30700-510b-4dff-9e99-cb775b6dd2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
